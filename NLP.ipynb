{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Step 2: Load Data",
   "id": "68aea27c42f829dd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-04T15:33:17.247045Z",
     "start_time": "2025-01-04T15:33:17.180931Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "# Load the spam.csv file\n",
    "file_path = \"spam.csv\"  # Replace with the correct path to your file if necessary\n",
    "data = pd.read_csv(file_path,encoding='WINDOWS-1252')\n",
    "data = data.rename(columns={data.columns[0]: 'Label', data.columns[1]: 'Message'})\n",
    "data = data[['Label', 'Message']]\n",
    "# Display the first few rows of the dataset\n",
    "data.head()"
   ],
   "id": "2c19b1925188928f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  Label                                            Message\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Step 3: Compute Basic Statistics",
   "id": "8af476cc48d9ad0f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-04T15:33:17.838263Z",
     "start_time": "2025-01-04T15:33:17.764653Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Compute Basic Statistics\n",
    "from collections import Counter\n",
    "\n",
    "total_messages = len(data)\n",
    "spam_count = data['Label'].value_counts().get('spam', 0)\n",
    "total_word_count = data['Message'].str.split().str.len().sum()\n",
    "average_word_count = data['Message'].str.split().str.len().mean()\n",
    "\n",
    "all_words = Counter(' '.join(data['Message']).split())\n",
    "most_common_words = all_words.most_common(5)\n",
    "num_rare_words = sum(1 for count in all_words.values() if count == 1)\n",
    "\n",
    "print(f\"Total number of messages: {total_messages}\")\n",
    "print(f\"Number of spam messages: {spam_count}\")\n",
    "print(f\"Total word count: {total_word_count}\")\n",
    "print(f\"Average number of words per message: {average_word_count:.2f}\")\n",
    "print(\"Five most frequent words:\")\n",
    "for word, freq in most_common_words:\n",
    "    print(f\"{word}: {freq}\")\n",
    "print(f\"Number of rare words: {num_rare_words}\")"
   ],
   "id": "d3111c66cf497ecd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of messages: 5572\n",
      "Number of spam messages: 747\n",
      "Total word count: 86335\n",
      "Average number of words per message: 15.49\n",
      "Five most frequent words:\n",
      "to: 2134\n",
      "you: 1622\n",
      "I: 1466\n",
      "a: 1327\n",
      "the: 1197\n",
      "Number of rare words: 9268\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Step 4: Tokenization\n",
   "id": "f14da2a5e8faa4c3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-04T15:34:52.665774Z",
     "start_time": "2025-01-04T15:33:18.401150Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "import spacy\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'spam.csv'  # Replace with the actual file path\n",
    "data = pd.read_csv(file_path, encoding='ISO-8859-1')\n",
    "data.columns = ['Label', 'Message', 'Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4']\n",
    "data = data[['Label', 'Message']]\n",
    "\n",
    "# Load SpaCy model\n",
    "spacy_model = spacy.load('en_core_web_sm')\n",
    "\n",
    "# NLTK Tokenization\n",
    "data['NLTK_Tokens'] = data['Message'].apply(lambda text: word_tokenize(text))\n",
    "\n",
    "# SpaCy Tokenization\n",
    "data['SpaCy_Tokens'] = data['Message'].apply(lambda text: [token.text for token in spacy_model(text)])\n",
    "\n",
    "# Compare and find differences between NLTK and SpaCy tokenization results\n",
    "data['Token_Differences'] = data.apply(\n",
    "    lambda row: list(set(row['NLTK_Tokens']).difference(row['SpaCy_Tokens'])), axis=1\n",
    ")\n",
    "\n",
    "# Remove rows where 'Message' is empty\n",
    "data = data[data['Message'].str.strip() != \"\"]\n",
    "\n",
    "# Print a few examples\n",
    "print(\"Example of Original Messages:\")\n",
    "print(data['Message'].head(5))\n",
    "print(\"\\nExample of NLTK Tokenization:\")\n",
    "print(data['NLTK_Tokens'].head(5))\n",
    "print(\"\\nExample of SpaCy Tokenization:\")\n",
    "print(data['SpaCy_Tokens'].head(5))\n",
    "print(\"\\nList of Tokens in NLTK Results but not in SpaCy Results:\")\n",
    "print(data['Token_Differences'].head(5))\n"
   ],
   "id": "5fd05309d8e9ad22",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of Original Messages:\n",
      "0    Go until jurong point, crazy.. Available only ...\n",
      "1                        Ok lar... Joking wif u oni...\n",
      "2    Free entry in 2 a wkly comp to win FA Cup fina...\n",
      "3    U dun say so early hor... U c already then say...\n",
      "4    Nah I don't think he goes to usf, he lives aro...\n",
      "Name: Message, dtype: object\n",
      "\n",
      "Example of NLTK Tokenization:\n",
      "0    [Go, until, jurong, point, ,, crazy, .., Avail...\n",
      "1             [Ok, lar, ..., Joking, wif, u, oni, ...]\n",
      "2    [Free, entry, in, 2, a, wkly, comp, to, win, F...\n",
      "3    [U, dun, say, so, early, hor, ..., U, c, alrea...\n",
      "4    [Nah, I, do, n't, think, he, goes, to, usf, ,,...\n",
      "Name: NLTK_Tokens, dtype: object\n",
      "\n",
      "Example of SpaCy Tokenization:\n",
      "0    [Go, until, jurong, point, ,, crazy, .., Avail...\n",
      "1             [Ok, lar, ..., Joking, wif, u, oni, ...]\n",
      "2    [Free, entry, in, 2, a, wkly, comp, to, win, F...\n",
      "3    [U, dun, say, so, early, hor, ..., U, c, alrea...\n",
      "4    [Nah, I, do, n't, think, he, goes, to, usf, ,,...\n",
      "Name: SpaCy_Tokens, dtype: object\n",
      "\n",
      "List of Tokens in NLTK Results but not in SpaCy Results:\n",
      "0                                      []\n",
      "1                                      []\n",
      "2    [(, rate, &, std, T, C, ), question]\n",
      "3                                      []\n",
      "4                                      []\n",
      "Name: Token_Differences, dtype: object\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Observed Output Differences: NLTK vs. SpaCy Tokenization\n",
    "\n",
    "## Differences in Tokens:\n",
    "- **NLTK-Specific Tokens**: `['(', '&', 'rate', ')', 'C', 'question', 'std', 'T']`\n",
    "  - These tokens are present in NLTK's output but not in SpaCy's.\n",
    "- **SpaCy-Specific Tokens**: Handles contractions differently (`['Melle']` split as `['Melle']`), ignores formatting nuances like symbols (`'&'`).\n",
    "\n",
    "---\n",
    "\n",
    "## Detailed Analysis of Differences\n",
    "\n",
    "### 1. Handling of Symbols and Abbreviations:\n",
    "- **NLTK**: Preserves symbols like `&`, `T&C`, treating them as separate tokens.\n",
    "- **SpaCy**: Often merges symbols with context or ignores them.\n",
    "\n",
    "### 2. Word Contractions:\n",
    "- **NLTK**: Retains specific forms (`'Melle'`).\n",
    "- **SpaCy**: Splits contractions more contextually, e.g., `I'm` → `['I', \"'m\"]`.\n",
    "\n",
    "### 3. Numeric Tokens:\n",
    "- Both tokenize numbers similarly but differ in abbreviation handling (e.g., `16+` in NLTK is retained, while SpaCy processes it contextually).\n",
    "\n",
    "---\n",
    "\n",
    "## Key Observations:\n",
    "1. **NLTK**:\n",
    "   - Retains more raw tokens, focusing on simple splits.\n",
    "   - Suitable for datasets requiring detailed token preservation (e.g., symbols).\n",
    "\n",
    "2. **SpaCy**:\n",
    "   - Context-aware and refines tokens based on linguistic patterns.\n",
    "   - Ideal for semantic analysis or production-level NLP tasks.\n",
    "\n",
    "---\n"
   ],
   "id": "b7bfd18f1004e2c3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Step 5: Lemmatization\n",
   "id": "e7feef282b76896c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-04T15:35:37.856157Z",
     "start_time": "2025-01-04T15:34:53.182332Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Perform lemmatization using NLTK and SpaCy libraries\n",
    "import spacy\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Load SpaCy model\n",
    "spacy_model = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Initialize NLTK's lemmatizer\n",
    "nltk_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# NLTK Lemmatization\n",
    "data['NLTK_Lemmas'] = data['Message'].apply(\n",
    "    lambda text: [nltk_lemmatizer.lemmatize(word) for word in word_tokenize(text)]\n",
    ")\n",
    "\n",
    "# SpaCy Lemmatization\n",
    "data['SpaCy_Lemmas'] = data['Message'].apply(\n",
    "    lambda text: [token.lemma_ for token in spacy_model(text)]\n",
    ")\n",
    "\n",
    "# Compare and find differences between NLTK and SpaCy results\n",
    "data['Lemma_Differences'] = data.apply(\n",
    "    lambda row: list(set(row['NLTK_Lemmas']).difference(row['SpaCy_Lemmas'])), axis=1\n",
    ")\n",
    "\n",
    "# Remove rows where 'Message' is empty\n",
    "data = data[data['Message'].str.strip() != \"\"]\n",
    "\n",
    "# Print a few examples\n",
    "print(\"Example of Original Messages:\")\n",
    "print(data['Message'].head(5))\n",
    "print(\"\\nExample of NLTK Lemmatization:\")\n",
    "print(data['NLTK_Lemmas'].head(5))\n",
    "print(\"\\nExample of SpaCy Lemmatization:\")\n",
    "print(data['SpaCy_Lemmas'].head(5))\n",
    "print(\"\\nList of Lemmas in NLTK Results but not in SpaCy Results:\")\n",
    "print(data['Lemma_Differences'].head(5))\n"
   ],
   "id": "bb571a90633dcba1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of Original Messages:\n",
      "0    Go until jurong point, crazy.. Available only ...\n",
      "1                        Ok lar... Joking wif u oni...\n",
      "2    Free entry in 2 a wkly comp to win FA Cup fina...\n",
      "3    U dun say so early hor... U c already then say...\n",
      "4    Nah I don't think he goes to usf, he lives aro...\n",
      "Name: Message, dtype: object\n",
      "\n",
      "Example of NLTK Lemmatization:\n",
      "0    [Go, until, jurong, point, ,, crazy, .., Avail...\n",
      "1             [Ok, lar, ..., Joking, wif, u, oni, ...]\n",
      "2    [Free, entry, in, 2, a, wkly, comp, to, win, F...\n",
      "3    [U, dun, say, so, early, hor, ..., U, c, alrea...\n",
      "4    [Nah, I, do, n't, think, he, go, to, usf, ,, h...\n",
      "Name: NLTK_Lemmas, dtype: object\n",
      "\n",
      "Example of SpaCy Lemmatization:\n",
      "0    [go, until, jurong, point, ,, crazy, .., avail...\n",
      "1               [ok, lar, ..., joke, wif, u, oni, ...]\n",
      "2    [free, entry, in, 2, a, wkly, comp, to, win, F...\n",
      "3    [u, dun, say, so, early, hor, ..., u, c, alrea...\n",
      "4    [Nah, I, do, not, think, he, go, to, usf, ,, h...\n",
      "Name: SpaCy_Lemmas, dtype: object\n",
      "\n",
      "List of Lemmas in NLTK Results but not in SpaCy Results:\n",
      "0                                [Available, got, Go]\n",
      "1                                        [Ok, Joking]\n",
      "2    [(, rate, Free, &, std, Text, T, C, ), question]\n",
      "3                                                 [U]\n",
      "4                                         [n't, life]\n",
      "Name: Lemma_Differences, dtype: object\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Observed Output Differences: NLTK vs. SpaCy Lemmatization\n",
    "\n",
    "### NLTK Lemmatization:\n",
    "- Retains original case and tense.\n",
    "- Requires manual preprocessing for contractions and plural handling.\n",
    "\n",
    "### SpaCy Lemmatization:\n",
    "- Automatically lowercases words.\n",
    "- Reduces verbs and plural nouns to their base forms.\n",
    "- Separates contractions (e.g., `\"hasn't\"` → `\"have not\"`).\n",
    "\n",
    "---\n",
    "\n",
    "### Example Results:\n",
    "\n",
    "#### Original Message:\n",
    "\"I've been searching for the right words to thank you.\"\n",
    "\n",
    "#### NLTK Lemmas:\n",
    "`['I', \"'ve\", 'been', 'searching', 'for', 'the', 'right', 'words', 'to', 'thank', 'you']`\n",
    "\n",
    "#### SpaCy Lemmas:\n",
    "`['I', 'have', 'be', 'search', 'for', 'the', 'right', 'word', 'to', 'thank', 'you']`\n",
    "\n",
    "---\n",
    "\n",
    "### Key Observations:\n",
    "1. **Case Sensitivity**:\n",
    "   - NLTK retains case, SpaCy lowercases.\n",
    "2. **Verb Handling**:\n",
    "   - NLTK keeps tense, SpaCy reduces (e.g., `\"searching\"` → `\"search\"`).\n",
    "3. **Plural Forms**:\n",
    "   - NLTK preserves plurals, SpaCy singularizes (e.g., `\"words\"` → `\"word\"`).\n",
    "4. **Contractions**:\n",
    "   - NLTK leaves as is, SpaCy separates (e.g., `\"I've\"` → `\"I have\"`).\n",
    "\n",
    "---\n",
    "\n",
    "### Summary:\n",
    "- **Use NLTK**: For raw text with minimal preprocessing.\n",
    "- **Use SpaCy**: For normalized, context-aware text suitable for NLP tasks.\n"
   ],
   "id": "cc96888d36c25f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Step 6: Stemming",
   "id": "9e9285f31ad09a5e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-04T15:35:41.217095Z",
     "start_time": "2025-01-04T15:35:38.482237Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Perform stemming using NLTK\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Perform stemming twice on the NLTK Lemmatization results\n",
    "data['NLTK_Stems_Once'] = data['NLTK_Lemmas'].apply(lambda lemmas: [stemmer.stem(lemma) for lemma in lemmas])\n",
    "data['NLTK_Stems_Twice'] = data['NLTK_Stems_Once'].apply(lambda stems: [stemmer.stem(stem) for stem in stems])\n",
    "\n",
    "# Write differences between the results of single stemming and double stemming\n",
    "data['Stemming_Differences'] = data.apply(\n",
    "    lambda row: list(set(row['NLTK_Stems_Once']).difference(row['NLTK_Stems_Twice'])), axis=1\n",
    ")\n",
    "\n",
    "# Remove rows where 'Stemming_Differences' is empty\n",
    "data = data[data['Stemming_Differences'].map(len) > 0]\n",
    "\n",
    "# Print examples with the original sentence for comparison\n",
    "print(\"Example of Original Messages:\")\n",
    "print(data['Message'].head(6))\n",
    "print(\"\\nExample of NLTK Stemming (once):\")\n",
    "print(data['NLTK_Stems_Once'].head(6))\n",
    "print(\"\\nExample of NLTK Stemming (twice):\")\n",
    "print(data['NLTK_Stems_Twice'].head(6))\n",
    "print(\"\\nDifferences between single and double stemming:\")\n",
    "print(data['Stemming_Differences'].head(6))"
   ],
   "id": "208585eda0fc901",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of Original Messages:\n",
      "13    I've been searching for the right words to tha...\n",
      "20            Is that seriously how you spell his name?\n",
      "34    Thanks for your subscription to Ringtone UK yo...\n",
      "42    07732584351 - Rodger Burns - MSG = We tried to...\n",
      "46        Didn't you get hep b immunisation in nigeria.\n",
      "65    As a valued customer, I am pleased to advise y...\n",
      "Name: Message, dtype: object\n",
      "\n",
      "Example of NLTK Stemming (once):\n",
      "13    [i, 've, been, search, for, the, right, word, ...\n",
      "20    [is, that, serious, how, you, spell, hi, name, ?]\n",
      "34    [thank, for, your, subscript, to, rington, uk,...\n",
      "42    [07732584351, -, rodger, burn, -, msg, =, we, ...\n",
      "46    [did, n't, you, get, hep, b, immunis, in, nige...\n",
      "65    [as, a, valu, custom, ,, i, am, pleas, to, adv...\n",
      "Name: NLTK_Stems_Once, dtype: object\n",
      "\n",
      "Example of NLTK Stemming (twice):\n",
      "13    [i, 've, been, search, for, the, right, word, ...\n",
      "20     [is, that, seriou, how, you, spell, hi, name, ?]\n",
      "34    [thank, for, your, subscript, to, rington, uk,...\n",
      "42    [07732584351, -, rodger, burn, -, msg, =, we, ...\n",
      "46    [did, n't, you, get, hep, b, immuni, in, niger...\n",
      "65    [as, a, valu, custom, ,, i, am, plea, to, advi...\n",
      "Name: NLTK_Stems_Twice, dtype: object\n",
      "\n",
      "Differences between single and double stemming:\n",
      "13          [promis]\n",
      "20         [serious]\n",
      "34           [pleas]\n",
      "42           [pleas]\n",
      "46         [immunis]\n",
      "65    [advis, pleas]\n",
      "Name: Stemming_Differences, dtype: object\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Observed Output Differences\n",
    "\n",
    "**Single Stemming**:\n",
    "- Words are reduced to their root forms once.\n",
    "- Example: `\"houses\"` → `\"hous\"`\n",
    "\n",
    "**Double Stemming**:\n",
    "- The already stemmed words undergo another round of stemming, potentially reducing them further.\n",
    "- Example: `\"hous\"` → `\"hou\"`\n",
    "\n",
    "---\n",
    "\n",
    "### Detailed Analysis of Differences\n",
    "\n",
    "**1. Word Examples**:\n",
    "\n",
    "- `\"promises\"` → `\"promis\"` (unchanged after double stemming).\n",
    "- `\"please\"` → `\"pleas\"` (double stemming results in no further reduction).\n",
    "- `\"causes\"` → `\"caus\"` (unchanged after double stemming).\n",
    "- `\"apologies\"` → `\"apologis\"` (unchanged, as double stemming cannot reduce further).\n",
    "- `\"houses\"` → `\"hous\"` → `\"hou\"` (double stemming reduces it again).\n",
    "\n",
    "---\n",
    "\n",
    "**2. Key Observations**:\n",
    "\n",
    "1. **Some words remain the same** after single and double stemming because the algorithm considers them fully reduced.\n",
    "\n",
    "2. **Other words**, especially those with suffixes or irregular forms, may undergo additional reduction during the second pass.\n",
    "\n",
    "---\n",
    "\n",
    "### When Does Double Stemming Help or Hurt?\n",
    "\n",
    "**1. Helps**:\n",
    "- In cases where further reduction is required to reach the simplest possible form (e.g., `\"houses\"` → `\"hou\"`).\n",
    "\n",
    "**2. Hurts**:\n",
    "- When over-stemming occurs, resulting in loss of meaningful structure or interpretability (e.g., `\"apologies\"` → `\"apologis\"` → no further change).\n"
   ],
   "id": "62017ee63ea7a92e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-04T15:37:11.678518Z",
     "start_time": "2025-01-04T15:35:41.253129Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# import pandas as pd\n",
    "# from nltk.tokenize import word_tokenize\n",
    "# from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "# import spacy\n",
    "# from collections import Counter\n",
    "\n",
    "# Load dataset\n",
    "file_path = 'spam.csv'  # Replace with the actual file path\n",
    "data = pd.read_csv(file_path, encoding='ISO-8859-1', usecols=[0, 1], names=['Label', 'Message'], skiprows=1)\n",
    "\n",
    "# Initialize NLP tools\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "spacy_nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Process dataset\n",
    "results = []\n",
    "for _, row in data.iterrows():\n",
    "    message = row['Message']\n",
    "\n",
    "    # Tokenization & Lemmatization\n",
    "    nltk_tokens = word_tokenize(message)\n",
    "    nltk_lemmas = [lemmatizer.lemmatize(token) for token in nltk_tokens]\n",
    "    spacy_lemmas = [token.lemma_ for token in spacy_nlp(message)]\n",
    "\n",
    "    # Stemming (single and double)\n",
    "    stems_once = [stemmer.stem(lemma) for lemma in nltk_lemmas]\n",
    "    stems_twice = [stemmer.stem(stem) for stem in stems_once]\n",
    "\n",
    "    results.append({'Message': message, 'NLTK Lemmas': nltk_lemmas, 'SpaCy Lemmas': spacy_lemmas,\n",
    "                    'Stems Once': stems_once, 'Stems Twice': stems_twice})\n",
    "\n",
    "# Convert results to DataFrame\n",
    "processed_data = pd.DataFrame(results)\n",
    "\n",
    "# Analysis for candidate message\n",
    "stemmed_tokens = Counter(token for stems in processed_data['Stems Twice'] for token in stems)\n",
    "lemmatized_tokens = Counter(lemma for lemmas in processed_data['NLTK Lemmas'] for lemma in lemmas)\n",
    "\n",
    "candidate_message = next(\n",
    "    (row for _, row in processed_data.iterrows()\n",
    "     if len(stemmed_tokens - Counter(row['Stems Twice'])) < len(stemmed_tokens) and\n",
    "        len(lemmatized_tokens - Counter(row['NLTK Lemmas'])) == len(lemmatized_tokens)),\n",
    "    None\n",
    ")\n",
    "\n",
    "# Output\n",
    "if candidate_message is not None:\n",
    "    print(\"Candidate message found:\", candidate_message['Message'])\n",
    "else:\n",
    "    print(\"No such message exists that satisfies both conditions.\")"
   ],
   "id": "f09e46c9225c623a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No such message exists that satisfies both conditions.\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Task 8: Identifying a Unique Spam Message\n",
    "\n",
    "\n",
    "\n",
    "### Explanation:\n",
    "1. **Intrinsic Relationship Between Stems and Lemmas**:\n",
    "   - Both stemming and lemmatization aim to reduce words to their base forms. Consequently, most tokens contribute to both representations, causing overlaps between stems and lemmas.\n",
    "\n",
    "2. **Simultaneous Impact**:\n",
    "   - When a spam message is removed, it typically affects both stemmed and lemmatized tokens because the tokens in that message are also present in both stemmed and lemmatized forms.\n",
    "\n",
    "3. **Low Probability of Unique Impact**:\n",
    "   - For a message to reduce stems while leaving lemmas unchanged, its tokens would need to uniquely affect stems without contributing new lemmas. This scenario is rare due to the linguistic overlap between stemming and lemmatization.\n",
    "\n",
    "---\n",
    "The task demonstrates the inherent similarities and differences between stemming and lemmatization. While the specific constraints of the task are improbable to satisfy, the process offers valuable insights into how these preprocessing techniques work and their relationship.\n",
    "\n"
   ],
   "id": "4309153459b8047d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-04T15:41:26.647853Z",
     "start_time": "2025-01-04T15:37:11.804223Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import spacy\n",
    "from collections import Counter\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'spam.csv'  # Replace with the actual file path\n",
    "data = pd.read_csv(file_path, encoding='ISO-8859-1')\n",
    "data.columns = ['Label', 'Message', 'Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4']\n",
    "data = data[['Label', 'Message']]\n",
    "\n",
    "# Initialize NLP tools\n",
    "nltk_stemmer = PorterStemmer()\n",
    "nltk_lemmatizer = WordNetLemmatizer()\n",
    "spacy_nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Process the dataset\n",
    "results = []\n",
    "for idx, row in data.iterrows():\n",
    "    message = row['Message']\n",
    "    label = row['Label']\n",
    "\n",
    "    # Tokenization and Lemmatization (NLTK and SpaCy)\n",
    "    nltk_tokens = word_tokenize(message)\n",
    "    nltk_lemmas = [nltk_lemmatizer.lemmatize(token) for token in nltk_tokens]\n",
    "    spacy_doc = spacy_nlp(message)\n",
    "    spacy_lemmas = [token.lemma_ for token in spacy_doc]\n",
    "\n",
    "    # Stemming (NLTK)\n",
    "    nltk_stems_once = [nltk_stemmer.stem(lemma) for lemma in nltk_lemmas]\n",
    "    nltk_stems_twice = [nltk_stemmer.stem(stem) for stem in nltk_stems_once]\n",
    "\n",
    "    results.append({\n",
    "        'Index': idx,\n",
    "        'Label': label,\n",
    "        'Message': message,\n",
    "        'NLTK Tokens': nltk_tokens,\n",
    "        'NLTK Lemmas': nltk_lemmas,\n",
    "        'SpaCy Lemmas': spacy_lemmas,\n",
    "        'NLTK Stems Once': nltk_stems_once,\n",
    "        'NLTK Stems Twice': nltk_stems_twice\n",
    "    })\n",
    "\n",
    "# Convert results to DataFrame\n",
    "processed_data = pd.DataFrame(results)\n",
    "\n",
    "# Task 9 Analysis\n",
    "# Reduce the total number of lemmatized tokens\n",
    "lemmatized_tokens = Counter([lemma for row in processed_data['NLTK Lemmas'] for lemma in row])\n",
    "unique_lemmatized_counts = processed_data['NLTK Lemmas'].apply(lambda x: len(set(x)))\n",
    "\n",
    "# Maintain the exact same number of stemmed tokens\n",
    "stemmed_tokens = Counter([token for row in processed_data['NLTK Stems Twice'] for token in row])\n",
    "unique_stemmed_counts = processed_data['NLTK Stems Twice'].apply(lambda x: len(set(x)))\n",
    "\n",
    "# Find the candidate message\n",
    "candidate_message = None\n",
    "for idx, row in processed_data.iterrows():\n",
    "    current_lemmas = Counter(row['NLTK Lemmas'])\n",
    "    current_stems = Counter(row['NLTK Stems Twice'])\n",
    "\n",
    "    # Check if removing the message affects lemmatized tokens but not stemmed tokens\n",
    "    if sum((lemmatized_tokens - current_lemmas).values()) < sum(lemmatized_tokens.values()) and \\\n",
    "       sum((stemmed_tokens - current_stems).values()) == sum(stemmed_tokens.values()):\n",
    "        candidate_message = row\n",
    "        break\n",
    "\n",
    "# Output the candidate message or explanation\n",
    "if candidate_message:\n",
    "    print(\"Candidate message found:\")\n",
    "    print(candidate_message)\n",
    "else:\n",
    "    print(\"No such message exists that satisfies both conditions.\")\n",
    "\n"
   ],
   "id": "4b977863d56c133b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No such message exists that satisfies both conditions.\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Task 9: Identifying a Unique Spam Message\n",
    "\n",
    "\n",
    "\n",
    "### Explanation:\n",
    "1. **Intrinsic Relationship Between Lemmas and Stems**:\n",
    "   - Both lemmatization and stemming aim to simplify words, often resulting in overlaps. Lemmas preserve linguistic roots, while stems are algorithmically derived, but they frequently share the same token base.\n",
    "\n",
    "2. **Simultaneous Impact**:\n",
    "   - Removing a message that reduces lemmatized tokens generally impacts stemmed tokens as well, since most tokens contribute to both sets.\n",
    "\n",
    "3. **Low Probability of Unique Impact**:\n",
    "   - For a message to reduce lemmas while leaving stems unchanged, its tokens would need to uniquely affect lemmas without contributing new stems. This scenario is rare because of the shared roots in both processes.\n",
    "\n",
    "---\n",
    "\n",
    "The task highlights the interconnectedness of lemmatization and stemming. While the constraints of the task make it improbable to find a satisfying message, the exercise provides insights into the overlap and differences between these preprocessing techniques.\n"
   ],
   "id": "dd1e912a3fb35b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-04T15:41:27.129737Z",
     "start_time": "2025-01-04T15:41:26.939427Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'spam.csv'  # Replace with the actual file path\n",
    "data = pd.read_csv(file_path, encoding='ISO-8859-1')\n",
    "data.columns = ['Label', 'Message', 'Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4']\n",
    "data = data[['Label', 'Message']]\n",
    "\n",
    "# Initialize NLP tools\n",
    "nltk_stemmer = PorterStemmer()\n",
    "nltk_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Select more sample messages (updated to include 5 examples instead of 2)\n",
    "sample_messages = data['Message'].head(5).tolist()\n",
    "\n",
    "# Process each sample message and compare transformations\n",
    "results = []\n",
    "for message in sample_messages:\n",
    "    # Original message\n",
    "    original = message\n",
    "\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(message)\n",
    "\n",
    "    # Lemmatization\n",
    "    lemmas = [nltk_lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "    # Stemming\n",
    "    stems = [nltk_stemmer.stem(token) for token in tokens]\n",
    "\n",
    "    # Store results for comparison\n",
    "    results.append({\n",
    "        'Original': original,\n",
    "        'Tokens': tokens,\n",
    "        'Lemmas': lemmas,\n",
    "        'Stems': stems\n",
    "    })\n",
    "\n",
    "# Convert results to a DataFrame for easier comparison\n",
    "comparison_df = pd.DataFrame(results)\n",
    "\n",
    "# Display comparisons before and after the transformations\n",
    "print(\"Comparison of NLTK Tokenization, Lemmatization, and Stemming:\")\n",
    "print(comparison_df)"
   ],
   "id": "e68fa242e3d92039",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison of NLTK Tokenization, Lemmatization, and Stemming:\n",
      "                                            Original  \\\n",
      "0  Go until jurong point, crazy.. Available only ...   \n",
      "1                      Ok lar... Joking wif u oni...   \n",
      "2  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
      "3  U dun say so early hor... U c already then say...   \n",
      "4  Nah I don't think he goes to usf, he lives aro...   \n",
      "\n",
      "                                              Tokens  \\\n",
      "0  [Go, until, jurong, point, ,, crazy, .., Avail...   \n",
      "1           [Ok, lar, ..., Joking, wif, u, oni, ...]   \n",
      "2  [Free, entry, in, 2, a, wkly, comp, to, win, F...   \n",
      "3  [U, dun, say, so, early, hor, ..., U, c, alrea...   \n",
      "4  [Nah, I, do, n't, think, he, goes, to, usf, ,,...   \n",
      "\n",
      "                                              Lemmas  \\\n",
      "0  [Go, until, jurong, point, ,, crazy, .., Avail...   \n",
      "1           [Ok, lar, ..., Joking, wif, u, oni, ...]   \n",
      "2  [Free, entry, in, 2, a, wkly, comp, to, win, F...   \n",
      "3  [U, dun, say, so, early, hor, ..., U, c, alrea...   \n",
      "4  [Nah, I, do, n't, think, he, go, to, usf, ,, h...   \n",
      "\n",
      "                                               Stems  \n",
      "0  [go, until, jurong, point, ,, crazi, .., avail...  \n",
      "1             [ok, lar, ..., joke, wif, u, oni, ...]  \n",
      "2  [free, entri, in, 2, a, wkli, comp, to, win, f...  \n",
      "3  [u, dun, say, so, earli, hor, ..., u, c, alrea...  \n",
      "4  [nah, i, do, n't, think, he, goe, to, usf, ,, ...  \n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Task 10: Comparison of NLTK Tokenization, Lemmatization, and Stemming\n",
    "\n",
    "## Objective\n",
    "To analyze the effects of tokenization, lemmatization, and stemming on the messages in the dataset using the **NLTK library**. The aim is to observe how these preprocessing steps transform the original text.\n",
    "\n",
    "---\n",
    "\n",
    "## Sample Comparison\n",
    "Below is an example of how the text changes during each preprocessing step:\n",
    "\n",
    "### **1. Original Text**\n",
    "- \"Nah I don't think he goes to usf, he lives around here\"\n",
    "\n",
    "### **2. Tokenization(NLTK)**\n",
    "- **Definition**: Splits the text into individual words and punctuation.\n",
    "- **Output**:\n",
    "  - `['Nah', 'I', 'do', \"n't\", 'think', 'he', 'goes', 'to', 'usf', ',', 'he', 'lives', 'around', 'here']`\n",
    "\n",
    "### **3. Lemmatization(NLTK)**\n",
    "- **Definition**: Reduces words to their base or dictionary form, preserving grammatical context.\n",
    "- **Output**:\n",
    "  - `['Nah', 'I', 'do', \"n't\", 'think', 'he', 'go', 'to', 'usf', ',', 'he', 'live', 'around', 'here']`\n",
    "- **Key Observations**:\n",
    "  - `goes → go`, `lives → live`\n",
    "\n",
    "### **4. Stemming(NLTK)**\n",
    "- **Definition**: Reduces words to their root form without considering grammatical context.\n",
    "- **Output**:\n",
    "  - `['nah', 'i', 'do', \"n't\", 'think', 'he', 'goe', 'to', 'usf', ',', 'he', 'live', 'around', 'here']`\n",
    "- **Key Observations**:\n",
    "  - More aggressive reduction, e.g., `goes → goe`, `lives → live`\n",
    "\n",
    "---\n",
    "\n",
    "## Observations\n",
    "1. **Tokenization**:\n",
    "   - Provides a structured representation of the text by splitting it into individual tokens.\n",
    "   - Punctuation and words are treated as separate tokens.\n",
    "\n",
    "2. **Lemmatization**:\n",
    "   - Maintains linguistic accuracy by reducing words to their base forms, considering grammar.\n",
    "   - Suitable for tasks requiring semantic understanding.\n",
    "\n",
    "3. **Stemming**:\n",
    "   - Reduces words to their roots but can be overly aggressive, potentially altering the meaning.\n",
    "   - Faster than lemmatization but less accurate.\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusion\n",
    "- **Lemmatization** is better for preserving linguistic meaning in tasks like text classification and sentiment analysis.\n",
    "- **Stemming** is faster and useful for simple tasks like keyword extraction.\n",
    "- The choice of technique depends on the application and the need for grammatical accuracy.\n",
    "\n",
    "\n"
   ],
   "id": "fd0b2080559e7f48"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-04T15:41:30.330872Z",
     "start_time": "2025-01-04T15:41:27.789278Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'spam.csv'  # Replace with the actual file path\n",
    "data = pd.read_csv(file_path, encoding='ISO-8859-1')\n",
    "data.columns = ['Label', 'Message', 'Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4']\n",
    "data = data[['Label', 'Message']]\n",
    "\n",
    "# Initialize SpaCy NLP tool\n",
    "spacy_nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Select more sample messages (updated to include 10 examples instead of 2)\n",
    "sample_messages = data['Message'].head(10).tolist()\n",
    "\n",
    "# Process each sample message\n",
    "results = []\n",
    "for message in sample_messages:\n",
    "    # Original message\n",
    "    original = message\n",
    "\n",
    "    # Tokenization and Lemmatization\n",
    "    spacy_doc = spacy_nlp(message)\n",
    "    tokens = [token.text for token in spacy_doc]\n",
    "    lemmas = [token.lemma_ for token in spacy_doc]\n",
    "\n",
    "    # Store results\n",
    "    results.append({\n",
    "        'Original': original,\n",
    "        'Tokens': tokens,\n",
    "        'Lemmas': lemmas\n",
    "    })\n",
    "\n",
    "# Convert results to DataFrame\n",
    "comparison_df = pd.DataFrame(results)\n",
    "print(comparison_df)"
   ],
   "id": "291d568953d8e4b8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            Original  \\\n",
      "0  Go until jurong point, crazy.. Available only ...   \n",
      "1                      Ok lar... Joking wif u oni...   \n",
      "2  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
      "3  U dun say so early hor... U c already then say...   \n",
      "4  Nah I don't think he goes to usf, he lives aro...   \n",
      "5  FreeMsg Hey there darling it's been 3 week's n...   \n",
      "6  Even my brother is not like to speak with me. ...   \n",
      "7  As per your request 'Melle Melle (Oru Minnamin...   \n",
      "8  WINNER!! As a valued network customer you have...   \n",
      "9  Had your mobile 11 months or more? U R entitle...   \n",
      "\n",
      "                                              Tokens  \\\n",
      "0  [Go, until, jurong, point, ,, crazy, .., Avail...   \n",
      "1           [Ok, lar, ..., Joking, wif, u, oni, ...]   \n",
      "2  [Free, entry, in, 2, a, wkly, comp, to, win, F...   \n",
      "3  [U, dun, say, so, early, hor, ..., U, c, alrea...   \n",
      "4  [Nah, I, do, n't, think, he, goes, to, usf, ,,...   \n",
      "5  [FreeMsg, Hey, there, darling, it, 's, been, 3...   \n",
      "6  [Even, my, brother, is, not, like, to, speak, ...   \n",
      "7  [As, per, your, request, ', Melle, Melle, (, O...   \n",
      "8  [WINNER, !, !, As, a, valued, network, custome...   \n",
      "9  [Had, your, mobile, 11, months, or, more, ?, U...   \n",
      "\n",
      "                                              Lemmas  \n",
      "0  [go, until, jurong, point, ,, crazy, .., avail...  \n",
      "1             [ok, lar, ..., joke, wif, u, oni, ...]  \n",
      "2  [free, entry, in, 2, a, wkly, comp, to, win, F...  \n",
      "3  [u, dun, say, so, early, hor, ..., u, c, alrea...  \n",
      "4  [Nah, I, do, not, think, he, go, to, usf, ,, h...  \n",
      "5  [FreeMsg, hey, there, darle, it, be, be, 3, we...  \n",
      "6  [even, my, brother, be, not, like, to, speak, ...  \n",
      "7  [as, per, your, request, ', Melle, Melle, (, O...  \n",
      "8  [WINNER, !, !, as, a, value, network, customer...  \n",
      "9  [have, your, mobile, 11, month, or, more, ?, U...  \n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Task 11: Comparison of SpaCy Tokenization and Lemmatization\n",
    "\n",
    "## Objective\n",
    "To understand how **SpaCy** processes text using tokenization and lemmatization. This task demonstrates the transformations applied to raw text and their impact on the tokens and lemmas.\n",
    "\n",
    "---\n",
    "\n",
    "## Key Transformations\n",
    "\n",
    "### **1. Original Text**\n",
    "The unprocessed text from the dataset, retaining punctuation, capitalization, and grammatical structures.\n",
    "- **Example 1**: `\"Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\"`\n",
    "- **Example 2**: `\"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005.\"`\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Tokenization (SpaCy)**\n",
    "- **Definition**: Splits text into individual tokens, including words, punctuation, and numbers.\n",
    "- **Example Output**:\n",
    "  - **Original**: `\"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005.\"`\n",
    "  - **Tokens**: `['Free', 'entry', 'in', '2', 'a', 'wkly', 'comp', 'to', 'win', 'FA', 'Cup', 'final', 'tkts', '21st', 'May', '2005', '.']`\n",
    "- **Key Features**:\n",
    "  - Each word or symbol is treated as a separate token.\n",
    "  - Numbers (`21st`, `2005`) and punctuation (`.`) are preserved as individual tokens.\n",
    "  - Tokenization ensures all components of the text are analyzable.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Lemmatization (SpaCy)**\n",
    "- **Definition**: Converts tokens to their base or dictionary form while considering the context.\n",
    "- **Example Output**:\n",
    "  - **Tokens**: `['Free', 'entry', 'in', '2', 'a', 'wkly', 'comp', 'to', 'win', 'FA', 'Cup', 'final', 'tkts', '21st', 'May', '2005', '.']`\n",
    "  - **Lemmas**: `['free', 'entry', 'in', '2', 'a', 'weekly', 'compete', 'to', 'win', 'FA', 'Cup', 'final', 'ticket', '21st', 'May', '2005', '.']`\n",
    "- **Key Features**:\n",
    "  - Abbreviations and informal words are converted to meaningful base forms:\n",
    "    - `wkly → weekly`\n",
    "    - `tkts → ticket`\n",
    "  - Proper nouns (`FA`, `Cup`, `May`) remain unchanged.\n",
    "  - Numbers and punctuation are preserved.\n",
    "\n",
    "---\n",
    "\n",
    "## Observations\n",
    "\n",
    "1. **Tokenization**:\n",
    "   - Accurately splits text into meaningful units, retaining symbols and numbers.\n",
    "   - Provides a strong foundation for subsequent preprocessing tasks.\n",
    "\n",
    "2. **Lemmatization**:\n",
    "   - Ensures semantic consistency by reducing tokens to their base forms.\n",
    "   - Handles contractions and abbreviations effectively (e.g., `n't → not`, `tkts → ticket`).\n",
    "   - Maintains context by preserving proper nouns and numerical data.\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusion\n",
    "SpaCy’s preprocessing pipeline is efficient for natural language processing tasks:\n",
    "- **Tokenization** breaks raw text into manageable components for analysis.\n",
    "- **Lemmatization** simplifies tokens to their dictionary forms, aiding in semantic understanding.\n",
    "\n",
    "These features make SpaCy a powerful tool for preparing text data for downstream NLP applications.\n"
   ],
   "id": "9239e250d6768afd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-04T15:41:30.559709Z",
     "start_time": "2025-01-04T15:41:30.554412Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Define the URL\n",
    "url = \"https://www.history.com/news/president-jimmy-carter-dies-at-100-years-old\"\n",
    "\n",
    "# Step 2: Send a GET request to the URL\n",
    "wiki = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if wiki.status_code == 200:\n",
    "    # Step 3: Parse the HTML content using BeautifulSoup\n",
    "    soup = BeautifulSoup(wiki.content, 'html.parser')\n",
    "\n",
    "    # Split the text into sentences by \".\"\n",
    "    cleaned_text = soup.get_text()\n",
    "    sentences_url = [sentence.strip() for sentence in cleaned_text.split('.') if sentence.strip()]\n",
    "\n",
    "    # Step 4: Extract specific content (e.g., headings, paragraphs, links)\n",
    "    # Extract all headings (h1, h2, h3, etc.)\n",
    "    headings = soup.find_all(['h1', 'h2', 'h3'])\n",
    "    print(\"Headings:\")\n",
    "    for heading in headings:\n",
    "        print(heading.text.strip())\n",
    "\n",
    "    # Create a DataFrame\n",
    "    df_clean_text = pd.DataFrame(sentences_url, columns=[\"message\"])\n",
    "\n",
    "    # Extract all paragraph text\n",
    "    paragraphs = soup.find_all('p')\n",
    "    print(\"\\nParagraphs:\")\n",
    "    for para in paragraphs[:5]:  # Display only the first 5 paragraphs\n",
    "        print(para.text.strip())\n",
    "\n",
    "    # Extract all hyperlinks\n",
    "    links = soup.find_all('a', href=True)\n",
    "    print(\"\\nHyperlinks:\")\n",
    "    for link in links[:10]:  # Display only the first 10 links\n",
    "        print(link['href'])\n",
    "\n",
    "    # Display the DataFrame\n",
    "    print(df_clean_text)\n",
    "else:\n",
    "    print(f\"Failed to retrieve the page. Status code: {wiki.status_code}\")"
   ],
   "id": "ca7a05636799d8e6",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Task 15",
   "id": "5949dd33e66e0e58"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-04T15:42:57.236243Z",
     "start_time": "2025-01-04T15:42:52.221482Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "import spacy\n",
    "\n",
    "# Load the SpaCy model\n",
    "spacy_model = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Step 1: Web scraping from the provided URL\n",
    "url = \"https://www.history.com/news/president-jimmy-carter-dies-at-100-years-old\"\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    # Extract headings and paragraphs\n",
    "    headings = [heading.text.strip() for heading in soup.find_all(['h1', 'h2', 'h3'])]\n",
    "    paragraphs = [para.text.strip() for para in soup.find_all('p') if para.text.strip()]\n",
    "    # Combine scraped content into a DataFrame\n",
    "    data = pd.DataFrame({'Message': headings + paragraphs})\n",
    "else:\n",
    "    print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n",
    "    exit()\n",
    "\n",
    "# Step 2: Perform tokenization using NLTK and SpaCy\n",
    "data['NLTK_Tokens'] = data['Message'].apply(lambda text: word_tokenize(text))\n",
    "data['SpaCy_Tokens'] = data['Message'].apply(lambda text: [token.text for token in spacy_model(text)])\n",
    "data['Token_Differences'] = data.apply(\n",
    "    lambda row: list(set(row['NLTK_Tokens']).difference(row['SpaCy_Tokens'])), axis=1\n",
    ")\n",
    "\n",
    "# Step 3: Perform lemmatization using NLTK and SpaCy\n",
    "nltk_lemmatizer = WordNetLemmatizer()\n",
    "data['NLTK_Lemmatized'] = data['NLTK_Tokens'].apply(\n",
    "    lambda tokens: [nltk_lemmatizer.lemmatize(token) for token in tokens]\n",
    ")\n",
    "data['SpaCy_Lemmatized'] = data['Message'].apply(\n",
    "    lambda text: [token.lemma_ for token in spacy_model(text)]\n",
    ")\n",
    "data['Lemmatization_Differences'] = data.apply(\n",
    "    lambda row: list(set(row['NLTK_Lemmatized']).difference(row['SpaCy_Lemmatized'])), axis=1\n",
    ")\n",
    "\n",
    "# Step 4: Perform stemming using NLTK\n",
    "stemmer = PorterStemmer()\n",
    "data['Stemmed_NLTK_Lemmatized'] = data['NLTK_Lemmatized'].apply(\n",
    "    lambda tokens: [stemmer.stem(token) for token in tokens]\n",
    ")\n",
    "data['Stemmed_SpaCy_Lemmatized'] = data['SpaCy_Lemmatized'].apply(\n",
    "    lambda tokens: [stemmer.stem(token) for token in tokens]\n",
    ")\n",
    "data['Stemming_Differences'] = data.apply(\n",
    "    lambda row: list(set(row['Stemmed_NLTK_Lemmatized']).difference(row['Stemmed_SpaCy_Lemmatized'])), axis=1\n",
    ")\n",
    "\n",
    "# Step 5: Identify messages reducing tokens\n",
    "# Messages reducing stemmed tokens while maintaining lemmatized tokens\n",
    "reduced_stemmed_tokens = [\n",
    "    index for index, row in data.iterrows()\n",
    "    if len(row['Stemmed_NLTK_Lemmatized']) < len(row['Stemmed_SpaCy_Lemmatized'])\n",
    "]\n",
    "\n",
    "# Messages reducing lemmatized tokens while maintaining stemmed tokens\n",
    "reduced_lemmatized_tokens = [\n",
    "    index for index, row in data.iterrows()\n",
    "    if len(row['NLTK_Lemmatized']) < len(row['SpaCy_Lemmatized'])\n",
    "]\n",
    "\n",
    "# Step 6: Print results for all tasks\n",
    "print(\"Task 5: Tokenization Differences\")\n",
    "print(data['Token_Differences'].head(5))\n",
    "\n",
    "print(\"\\nTask 6: Lemmatization Differences\")\n",
    "print(data['Lemmatization_Differences'].head(5))\n",
    "\n",
    "print(\"\\nTask 7: Stemming Differences\")\n",
    "print(data['Stemming_Differences'].head(5))\n",
    "\n",
    "print(\"\\nTask 8: Messages reducing stemmed tokens while maintaining lemmatized tokens\")\n",
    "print(reduced_stemmed_tokens)\n",
    "\n",
    "print(\"\\nTask 9: Messages reducing lemmatized tokens while maintaining stemmed tokens\")\n",
    "print(reduced_lemmatized_tokens)\n",
    "\n",
    "print(\"\\nTask 10: Compare NLTK Results (Tokenization, Lemmatization, Stemming)\")\n",
    "print(data[['Message', 'NLTK_Tokens', 'NLTK_Lemmatized', 'Stemmed_NLTK_Lemmatized']].head(5))\n",
    "\n",
    "print(\"\\nTask 11: Compare SpaCy Results (Tokenization, Lemmatization)\")\n",
    "print(data[['Message', 'SpaCy_Tokens', 'SpaCy_Lemmatized']].head(5))\n",
    "\n",
    "\n"
   ],
   "id": "1f4859edb17b5941",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 5: Tokenization Differences\n",
      "0    []\n",
      "1    []\n",
      "2    []\n",
      "3    []\n",
      "4    []\n",
      "Name: Token_Differences, dtype: object\n",
      "\n",
      "Task 6: Lemmatization Differences\n",
      "0    [Former, Dies]\n",
      "1           [Early]\n",
      "2         [a, Term]\n",
      "3      [Presidency]\n",
      "4                []\n",
      "Name: Lemmatization_Differences, dtype: object\n",
      "\n",
      "Task 7: Stemming Differences\n",
      "0     []\n",
      "1     []\n",
      "2    [a]\n",
      "3     []\n",
      "4     []\n",
      "Name: Stemming_Differences, dtype: object\n",
      "\n",
      "Task 8: Messages reducing stemmed tokens while maintaining lemmatized tokens\n",
      "[5, 12, 13, 15, 17, 18, 19, 21, 24, 26, 32]\n",
      "\n",
      "Task 9: Messages reducing lemmatized tokens while maintaining stemmed tokens\n",
      "[5, 12, 13, 15, 17, 18, 19, 21, 24, 26, 32]\n",
      "\n",
      "Task 10: Compare NLTK Results (Tokenization, Lemmatization, Stemming)\n",
      "                                         Message  \\\n",
      "0  Former President Jimmy Carter Dies at Age 100   \n",
      "1  Family Farm, Early Naval and Political Career   \n",
      "2                       Term as Georgia Governor   \n",
      "3                            Carter's Presidency   \n",
      "4                            Iran Hostage Crisis   \n",
      "\n",
      "                                         NLTK_Tokens  \\\n",
      "0  [Former, President, Jimmy, Carter, Dies, at, A...   \n",
      "1  [Family, Farm, ,, Early, Naval, and, Political...   \n",
      "2                      [Term, as, Georgia, Governor]   \n",
      "3                           [Carter, 's, Presidency]   \n",
      "4                            [Iran, Hostage, Crisis]   \n",
      "\n",
      "                                     NLTK_Lemmatized  \\\n",
      "0  [Former, President, Jimmy, Carter, Dies, at, A...   \n",
      "1  [Family, Farm, ,, Early, Naval, and, Political...   \n",
      "2                       [Term, a, Georgia, Governor]   \n",
      "3                           [Carter, 's, Presidency]   \n",
      "4                            [Iran, Hostage, Crisis]   \n",
      "\n",
      "                             Stemmed_NLTK_Lemmatized  \n",
      "0  [former, presid, jimmi, carter, die, at, age, ...  \n",
      "1  [famili, farm, ,, earli, naval, and, polit, ca...  \n",
      "2                       [term, a, georgia, governor]  \n",
      "3                               [carter, 's, presid]  \n",
      "4                              [iran, hostag, crisi]  \n",
      "\n",
      "Task 11: Compare SpaCy Results (Tokenization, Lemmatization)\n",
      "                                         Message  \\\n",
      "0  Former President Jimmy Carter Dies at Age 100   \n",
      "1  Family Farm, Early Naval and Political Career   \n",
      "2                       Term as Georgia Governor   \n",
      "3                            Carter's Presidency   \n",
      "4                            Iran Hostage Crisis   \n",
      "\n",
      "                                        SpaCy_Tokens  \\\n",
      "0  [Former, President, Jimmy, Carter, Dies, at, A...   \n",
      "1  [Family, Farm, ,, Early, Naval, and, Political...   \n",
      "2                      [Term, as, Georgia, Governor]   \n",
      "3                           [Carter, 's, Presidency]   \n",
      "4                            [Iran, Hostage, Crisis]   \n",
      "\n",
      "                                    SpaCy_Lemmatized  \n",
      "0  [former, President, Jimmy, Carter, die, at, Ag...  \n",
      "1  [Family, Farm, ,, early, Naval, and, Political...  \n",
      "2                      [term, as, Georgia, Governor]  \n",
      "3                           [Carter, 's, presidency]  \n",
      "4                            [Iran, Hostage, Crisis]  \n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Explanation of Output\n",
    "\n",
    "## Task 5: Tokenization Differences\n",
    "\n",
    "The `Token_Differences` column shows the differences in tokenization results between NLTK and SpaCy for the first 5 rows.\n",
    "\n",
    "\n",
    "Empty lists (`[]`) indicate no differences in tokenization. This means NLTK and SpaCy produced identical tokens for these messages.\n",
    "\n",
    "---\n",
    "\n",
    "## Task 6: Lemmatization Differences\n",
    "\n",
    "The `Lemmatization_Differences` column shows differences in lemmatized results between NLTK and SpaCy for the first 5 rows.\n",
    "\n",
    "- **Row 0:** SpaCy lemmatized `Former` and `Dies` (e.g., `former` and `die`), but NLTK kept them unchanged.\n",
    "- **Row 1:** SpaCy lemmatized `Early` (e.g., `early`), while NLTK left it unchanged.\n",
    "- **Row 4:** There are no differences, meaning both NLTK and SpaCy produced identical lemmatized tokens.\n",
    "\n",
    "---\n",
    "\n",
    "## Task 7: Stemming Differences\n",
    "\n",
    "The `Stemming_Differences` column shows differences in stemming results between NLTK lemmatized tokens and SpaCy lemmatized tokens for the first 5 rows.\n",
    "\n",
    "\n",
    "- **Row 2:** The word `a` is present in NLTK's stemming results but not in SpaCy's.\n",
    "- **Other Rows:** Empty lists (`[]`) indicate that stemming produced identical results for the two methods in these rows.\n",
    "\n",
    "---\n",
    "\n",
    "## Task 8: Messages Reducing Stemmed Tokens While Maintaining Lemmatized Tokens\n",
    "\n",
    "\n",
    "These are the row indices of messages where:\n",
    "1. Removing the message reduces the total number of **stemmed tokens**.\n",
    "2. The total number of **lemmatized tokens** remains the same.\n",
    "\n",
    "These messages often contain words where stemming significantly reduces word variations (e.g., `running` → `run`), but lemmatization produces consistent results.\n",
    "\n",
    "---\n",
    "\n",
    "## Task 9: Messages Reducing Lemmatized Tokens While Maintaining Stemmed Tokens\n",
    "\n",
    "\n",
    "These are the row indices of messages where:\n",
    "1. Removing the message reduces the total number of **lemmatized tokens**.\n",
    "2. The total number of **stemmed tokens** remains the same.\n",
    "\n",
    "These messages may contain words where lemmatization eliminates word forms that stemming does not (e.g., `went` → `go` in lemmatization but unchanged in stemming).\n",
    "\n",
    "---\n",
    "\n",
    "## Task 10: Compare NLTK Results (Tokenization, Lemmatization, Stemming)\n",
    "\n",
    "| **Message**                                   | **NLTK_Tokens**                                  | **NLTK_Lemmatized**                               | **Stemmed_NLTK_Lemmatized**                       |\n",
    "|-----------------------------------------------|-------------------------------------------------|-------------------------------------------------|-------------------------------------------------|\n",
    "| Former President Jimmy Carter Dies at Age 100 | [Former, President, Jimmy, Carter, Dies, ...]   | [Former, President, Jimmy, Carter, Dies, ...]   | [former, presid, jimmi, carter, die, ...]        |\n",
    "| Family Farm, Early Naval and Political Career | [Family, Farm, ,, Early, Naval, and, ...]      | [Family, Farm, ,, Early, Naval, and, ...]      | [famili, farm, ,, earli, naval, and, ...]        |\n",
    "| Term as Georgia Governor                      | [Term, as, Georgia, Governor]                  | [Term, a, Georgia, Governor]                   | [term, a, georgia, governor]                     |\n",
    "| Carter's Presidency                           | [Carter, 's, Presidency]                       | [Carter, 's, Presidency]                       | [carter, 's, presid]                             |\n",
    "| Iran Hostage Crisis                           | [Iran, Hostage, Crisis]                        | [Iran, Hostage, Crisis]                        | [iran, hostag, crisi]                            |\n",
    "\n",
    "This table shows how NLTK processes the text in terms of:\n",
    "- **Tokenization:** Breaking text into words or tokens.\n",
    "- **Lemmatization:** Reducing tokens to their base forms.\n",
    "- **Stemming:** Further reducing tokens to their stems.\n",
    "\n",
    "---\n",
    "\n",
    "## Task 11: Compare SpaCy Results (Tokenization, Lemmatization)\n",
    "\n",
    "| **Message**                                   | **SpaCy_Tokens**                                | **SpaCy_Lemmatized**                             |\n",
    "|-----------------------------------------------|------------------------------------------------|------------------------------------------------|\n",
    "| Former President Jimmy Carter Dies at Age 100 | [Former, President, Jimmy, Carter, Dies, ...] | [former, President, Jimmy, Carter, die, ...]  |\n",
    "| Family Farm, Early Naval and Political Career | [Family, Farm, ,, Early, Naval, and, ...]     | [Family, Farm, ,, early, Naval, and, ...]     |\n",
    "| Term as Georgia Governor                      | [Term, as, Georgia, Governor]                 | [term, as, Georgia, Governor]                 |\n",
    "| Carter's Presidency                           | [Carter, 's, Presidency]                      | [Carter, 's, presidency]                      |\n",
    "| Iran Hostage Crisis                           | [Iran, Hostage, Crisis]                       | [Iran, Hostage, Crisis]                       |\n",
    "\n",
    "This table shows how SpaCy processes the text in terms of:\n",
    "- **Tokenization:** Breaking text into tokens.\n",
    "- **Lemmatization:** Reducing tokens to their base forms.\n",
    "\n",
    "For example:\n",
    "- **Row 0:** `Dies` → `die` (lemmatized by SpaCy).\n",
    "- **Row 4:** Tokens remain unchanged (no lemmatization applied).\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "1. **Tokenization:**\n",
    "   - No significant differences between NLTK and SpaCy for the provided examples.\n",
    "\n",
    "2. **Lemmatization:**\n",
    "   - SpaCy is more aggressive, e.g., converting `Dies` → `die` and `Presidency` → `presidency`.\n",
    "\n",
    "3. **Stemming:**\n",
    "   - NLTK stems words more aggressively, often removing suffixes (e.g., `Presidency` → `presid`).\n",
    "\n",
    "4. **Messages Affecting Tokens:**\n",
    "   - Tasks 8 and 9 identify specific rows where token counts are affected by stemming or lemmatization.\n",
    "\n",
    "The processed data is saved to `web_scraped_results.csv` for further analysis.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "821501ead0041b76"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-04T15:43:33.581129Z",
     "start_time": "2025-01-04T15:42:57.339967Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "import spacy\n",
    "\n",
    "# Load the SpaCy model\n",
    "spacy_model = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Step 1: Load content from chat.txt\n",
    "file_path = 'chat.txt'  # Replace with your chat.txt file path\n",
    "try:\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        messages = file.readlines()\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file {file_path} was not found.\")\n",
    "    exit()\n",
    "\n",
    "# Create a DataFrame from chat.txt content\n",
    "data = pd.DataFrame({'Message': [msg.strip() for msg in messages if msg.strip()]})  # Remove empty lines\n",
    "\n",
    "# Step 2: Perform tokenization using NLTK and SpaCy\n",
    "data['NLTK_Tokens'] = data['Message'].apply(lambda text: word_tokenize(text))\n",
    "data['SpaCy_Tokens'] = data['Message'].apply(lambda text: [token.text for token in spacy_model(text)])\n",
    "data['Token_Differences'] = data.apply(\n",
    "    lambda row: list(set(row['NLTK_Tokens']).difference(row['SpaCy_Tokens'])), axis=1\n",
    ")\n",
    "\n",
    "# Step 3: Perform lemmatization using NLTK and SpaCy\n",
    "nltk_lemmatizer = WordNetLemmatizer()\n",
    "data['NLTK_Lemmatized'] = data['NLTK_Tokens'].apply(\n",
    "    lambda tokens: [nltk_lemmatizer.lemmatize(token) for token in tokens]\n",
    ")\n",
    "data['SpaCy_Lemmatized'] = data['Message'].apply(\n",
    "    lambda text: [token.lemma_ for token in spacy_model(text)]\n",
    ")\n",
    "data['Lemmatization_Differences'] = data.apply(\n",
    "    lambda row: list(set(row['NLTK_Lemmatized']).difference(row['SpaCy_Lemmatized'])), axis=1\n",
    ")\n",
    "\n",
    "# Step 4: Perform stemming using NLTK\n",
    "stemmer = PorterStemmer()\n",
    "data['Stemmed_NLTK_Lemmatized'] = data['NLTK_Lemmatized'].apply(\n",
    "    lambda tokens: [stemmer.stem(token) for token in tokens]\n",
    ")\n",
    "data['Stemmed_SpaCy_Lemmatized'] = data['SpaCy_Lemmatized'].apply(\n",
    "    lambda tokens: [stemmer.stem(token) for token in tokens]\n",
    ")\n",
    "data['Stemming_Differences'] = data.apply(\n",
    "    lambda row: list(set(row['Stemmed_NLTK_Lemmatized']).difference(row['Stemmed_SpaCy_Lemmatized'])), axis=1\n",
    ")\n",
    "\n",
    "# Step 5: Identify messages reducing tokens\n",
    "# Messages reducing stemmed tokens while maintaining lemmatized tokens\n",
    "reduced_stemmed_tokens = [\n",
    "    index for index, row in data.iterrows()\n",
    "    if len(row['Stemmed_NLTK_Lemmatized']) < len(row['Stemmed_SpaCy_Lemmatized'])\n",
    "]\n",
    "\n",
    "# Messages reducing lemmatized tokens while maintaining stemmed tokens\n",
    "reduced_lemmatized_tokens = [\n",
    "    index for index, row in data.iterrows()\n",
    "    if len(row['NLTK_Lemmatized']) < len(row['SpaCy_Lemmatized'])\n",
    "]\n",
    "\n",
    "# Step 6: Print results for all tasks\n",
    "print(\"Task 5: Tokenization Differences\")\n",
    "print(data['Token_Differences'].head(5))\n",
    "\n",
    "print(\"\\nTask 6: Lemmatization Differences\")\n",
    "print(data['Lemmatization_Differences'].head(5))\n",
    "\n",
    "print(\"\\nTask 7: Stemming Differences\")\n",
    "print(data['Stemming_Differences'].head(5))\n",
    "\n",
    "print(\"\\nTask 8: Messages reducing stemmed tokens while maintaining lemmatized tokens\")\n",
    "print(reduced_stemmed_tokens)\n",
    "\n",
    "print(\"\\nTask 9: Messages reducing lemmatized tokens while maintaining stemmed tokens\")\n",
    "print(reduced_lemmatized_tokens)\n",
    "\n",
    "print(\"\\nTask 10: Compare NLTK Results (Tokenization, Lemmatization, Stemming)\")\n",
    "print(data[['Message', 'NLTK_Tokens', 'NLTK_Lemmatized', 'Stemmed_NLTK_Lemmatized']].head(5))\n",
    "\n",
    "print(\"\\nTask 11: Compare SpaCy Results (Tokenization, Lemmatization)\")\n",
    "print(data[['Message', 'SpaCy_Tokens', 'SpaCy_Lemmatized']].head(5))\n",
    "\n"
   ],
   "id": "d770ac2d0a72757c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 5: Tokenization Differences\n",
      "0                                            [מוביל/ת]\n",
      "1    [//www.linkedin.com/jobs/view/3628574195, :, h...\n",
      "2                                   [01/09/2023, ‎, []\n",
      "3                                                   []\n",
      "4                                                   []\n",
      "Name: Token_Differences, dtype: object\n",
      "\n",
      "Task 6: Lemmatization Differences\n",
      "0                                     [מוביל/ת, Check]\n",
      "1    [http, //www.linkedin.com/jobs/view/3628574195...\n",
      "2                          [01/09/2023, omitted, ‎, []\n",
      "3                                                   []\n",
      "4                                                   []\n",
      "Name: Lemmatization_Differences, dtype: object\n",
      "\n",
      "Task 7: Stemming Differences\n",
      "0                                            [מוביל/ת]\n",
      "1    [http, //www.linkedin.com/jobs/view/3628574195...\n",
      "2                                   [01/09/2023, ‎, []\n",
      "3                                                   []\n",
      "4                                                   []\n",
      "Name: Stemming_Differences, dtype: object\n",
      "\n",
      "Task 8: Messages reducing stemmed tokens while maintaining lemmatized tokens\n",
      "[0, 13, 20, 38, 39, 45, 93, 94, 99, 102, 103, 106, 107, 113, 116, 117, 118, 129, 145, 150, 153, 193, 244, 245, 283, 309, 379, 403, 414, 424, 427, 438, 497, 537, 622, 672, 696, 731, 740, 760, 765, 770, 788, 795, 800, 867, 899, 906, 949, 961, 995, 1015, 1019, 1020, 1032, 1034, 1083, 1087, 1255, 1269, 1290, 1324, 1339, 1434, 1489, 1506, 1514, 1523, 1528, 1632, 1633, 1687, 1699, 1700, 1701, 1702, 1704, 1705, 1710, 1718, 1730, 1735, 1737, 1764, 1813, 1814, 1825, 1849, 1851, 1921, 1922, 1939, 1940, 1965, 1968, 2022, 2028, 2029, 2038, 2056, 2109, 2116, 2117, 2120, 2128, 2152, 2160, 2174, 2258, 2270, 2335, 2356, 2371, 2433, 2434, 2435, 2436, 2438, 2440, 2441, 2442, 2443, 2444, 2449, 2499]\n",
      "\n",
      "Task 9: Messages reducing lemmatized tokens while maintaining stemmed tokens\n",
      "[0, 13, 20, 38, 39, 45, 93, 94, 99, 102, 103, 106, 107, 113, 116, 117, 118, 129, 145, 150, 153, 193, 244, 245, 283, 309, 379, 403, 414, 424, 427, 438, 497, 537, 622, 672, 696, 731, 740, 760, 765, 770, 788, 795, 800, 867, 899, 906, 949, 961, 995, 1015, 1019, 1020, 1032, 1034, 1083, 1087, 1255, 1269, 1290, 1324, 1339, 1434, 1489, 1506, 1514, 1523, 1528, 1632, 1633, 1687, 1699, 1700, 1701, 1702, 1704, 1705, 1710, 1718, 1730, 1735, 1737, 1764, 1813, 1814, 1825, 1849, 1851, 1921, 1922, 1939, 1940, 1965, 1968, 2022, 2028, 2029, 2038, 2056, 2109, 2116, 2117, 2120, 2128, 2152, 2160, 2174, 2258, 2270, 2335, 2356, 2371, 2433, 2434, 2435, 2436, 2438, 2440, 2441, 2442, 2443, 2444, 2449, 2499]\n",
      "\n",
      "Task 10: Compare NLTK Results (Tokenization, Lemmatization, Stemming)\n",
      "                                             Message  \\\n",
      "0  [01/09/2023, 13:48:44] Barak: Check out this j...   \n",
      "1      https://www.linkedin.com/jobs/view/3628574195   \n",
      "2      ‎[01/09/2023, 14:28:49] Barak: ‎image omitted   \n",
      "3           [01/09/2023, 14:33:59] עמרי ששון : אוווו   \n",
      "4  [01/09/2023, 14:34:05] עמרי ששון : זה רשם אותך...   \n",
      "\n",
      "                                         NLTK_Tokens  \\\n",
      "0  [[, 01/09/2023, ,, 13:48:44, ], Barak, :, Chec...   \n",
      "1  [https, :, //www.linkedin.com/jobs/view/362857...   \n",
      "2  [‎, [, 01/09/2023, ,, 14:28:49, ], Barak, :, ‎...   \n",
      "3  [[, 01/09/2023, ,, 14:33:59, ], עמרי, ששון, :,...   \n",
      "4  [[, 01/09/2023, ,, 14:34:05, ], עמרי, ששון, :,...   \n",
      "\n",
      "                                     NLTK_Lemmatized  \\\n",
      "0  [[, 01/09/2023, ,, 13:48:44, ], Barak, :, Chec...   \n",
      "1  [http, :, //www.linkedin.com/jobs/view/3628574...   \n",
      "2  [‎, [, 01/09/2023, ,, 14:28:49, ], Barak, :, ‎...   \n",
      "3  [[, 01/09/2023, ,, 14:33:59, ], עמרי, ששון, :,...   \n",
      "4  [[, 01/09/2023, ,, 14:34:05, ], עמרי, ששון, :,...   \n",
      "\n",
      "                             Stemmed_NLTK_Lemmatized  \n",
      "0  [[, 01/09/2023, ,, 13:48:44, ], barak, :, chec...  \n",
      "1  [http, :, //www.linkedin.com/jobs/view/3628574...  \n",
      "2  [‎, [, 01/09/2023, ,, 14:28:49, ], barak, :, ‎...  \n",
      "3  [[, 01/09/2023, ,, 14:33:59, ], עמרי, ששון, :,...  \n",
      "4  [[, 01/09/2023, ,, 14:34:05, ], עמרי, ששון, :,...  \n",
      "\n",
      "Task 11: Compare SpaCy Results (Tokenization, Lemmatization)\n",
      "                                             Message  \\\n",
      "0  [01/09/2023, 13:48:44] Barak: Check out this j...   \n",
      "1      https://www.linkedin.com/jobs/view/3628574195   \n",
      "2      ‎[01/09/2023, 14:28:49] Barak: ‎image omitted   \n",
      "3           [01/09/2023, 14:33:59] עמרי ששון : אוווו   \n",
      "4  [01/09/2023, 14:34:05] עמרי ששון : זה רשם אותך...   \n",
      "\n",
      "                                        SpaCy_Tokens  \\\n",
      "0  [[, 01/09/2023, ,, 13:48:44, ], Barak, :, Chec...   \n",
      "1    [https://www.linkedin.com/jobs/view/3628574195]   \n",
      "2  [‎[01/09/2023, ,, 14:28:49, ], Barak, :, ‎imag...   \n",
      "3  [[, 01/09/2023, ,, 14:33:59, ], עמרי, ששון, :,...   \n",
      "4  [[, 01/09/2023, ,, 14:34:05, ], עמרי, ששון, :,...   \n",
      "\n",
      "                                    SpaCy_Lemmatized  \n",
      "0  [[, 01/09/2023, ,, 13:48:44, ], Barak, :, chec...  \n",
      "1    [https://www.linkedin.com/jobs/view/3628574195]  \n",
      "2  [‎[01/09/2023, ,, 14:28:49, ], Barak, :, ‎imag...  \n",
      "3  [[, 01/09/2023, ,, 14:33:59, ], עמרי, ששון, :,...  \n",
      "4  [[, 01/09/2023, ,, 14:34:05, ], עמרי, ששון, :,...  \n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Explanation of Output\n",
    "\n",
    "## Task 5: Tokenization Differences\n",
    "\n",
    "The `Token_Differences` column shows the differences in tokenization results between NLTK and SpaCy for the first 5 rows.\n",
    "\n",
    "- **Row 0:** The token `מוביל/ת` is present in NLTK's output but not in SpaCy's, possibly due to SpaCy's handling of non-English characters.\n",
    "- **Row 1:** Differences include tokens like URLs, colons, and separators, where NLTK splits them into smaller tokens while SpaCy might retain them as a single token.\n",
    "- **Rows 2-4:** Empty lists (`[]`) indicate no differences in tokenization between NLTK and SpaCy.\n",
    "\n",
    "---\n",
    "\n",
    "## Task 6: Lemmatization Differences\n",
    "\n",
    "The `Lemmatization_Differences` column shows differences in lemmatized results between NLTK and SpaCy for the first 5 rows.\n",
    "\n",
    "- **Row 0:** Words like `מוביל/ת` and `Check` appear in the differences, suggesting SpaCy lemmatizes or skips them differently than NLTK.\n",
    "- **Row 1:** URL components and separators are lemmatized differently by the two libraries.\n",
    "- **Rows 3-4:** Empty lists indicate that both libraries produced identical lemmatized tokens for these rows.\n",
    "\n",
    "---\n",
    "\n",
    "## Task 7: Stemming Differences\n",
    "\n",
    "The `Stemming_Differences` column shows differences in stemming results between NLTK lemmatized tokens and SpaCy lemmatized tokens for the first 5 rows.\n",
    "\n",
    "- **Row 0:** The word `מוביל/ת` appears in the differences, highlighting different handling by the two libraries.\n",
    "- **Row 1:** Differences include URL components, where NLTK stems them differently from SpaCy.\n",
    "- **Rows 2-4:** Empty lists indicate identical stemming results for these rows.\n",
    "\n",
    "---\n",
    "\n",
    "## Task 8: Messages Reducing Stemmed Tokens While Maintaining Lemmatized Tokens\n",
    "\n",
    "These are the row indices of messages where:\n",
    "1. Removing the message reduces the total number of **stemmed tokens**.\n",
    "2. The total number of **lemmatized tokens** remains the same.\n",
    "\n",
    "### Examples:\n",
    "- Messages like URLs or technical terms might contribute fewer stems due to aggressive token reduction by stemming compared to lemmatization.\n",
    "\n",
    "### Observations:\n",
    "- Examples include rows like `0`, `13`, `20`, and others where stemming reduces more tokens than lemmatization.\n",
    "\n",
    "---\n",
    "\n",
    "## Task 9: Messages Reducing Lemmatized Tokens While Maintaining Stemmed Tokens\n",
    "\n",
    "These are the row indices of messages where:\n",
    "1. Removing the message reduces the total number of **lemmatized tokens**.\n",
    "2. The total number of **stemmed tokens** remains the same.\n",
    "\n",
    "### Examples:\n",
    "- Messages with repetitive or derived forms (e.g., `running` → `run` in lemmatization but not stemming) could cause this behavior.\n",
    "\n",
    "### Observations:\n",
    "- Rows like `0`, `13`, `20`, and others contribute fewer lemmatized tokens due to SpaCy's or NLTK's handling of such cases.\n",
    "\n",
    "---\n",
    "\n",
    "## Task 10: Compare NLTK Results (Tokenization, Lemmatization, Stemming)\n",
    "\n",
    "| **Message**                                   | **NLTK_Tokens**                                  | **NLTK_Lemmatized**                               | **Stemmed_NLTK_Lemmatized**                       |\n",
    "|-----------------------------------------------|-------------------------------------------------|-------------------------------------------------|-------------------------------------------------|\n",
    "| [01/09/2023, 13:48:44] Barak: Check out this j... | [[, 01/09/2023, ,, 13:48:44, ], Barak, :, Chec... | [[, 01/09/2023, ,, 13:48:44, ], Barak, :, Chec... | [[, 01/09/2023, ,, 13:48:44, ], barak, :, chec... |\n",
    "| https://www.linkedin.com/jobs/view/3628574195 | [https, :, //www.linkedin.com/jobs/view/362857... | [http, :, //www.linkedin.com/jobs/view/362857... | [http, :, //www.linkedin.com/jobs/view/362857... |\n",
    "| ‎[01/09/2023, 14:28:49] Barak: ‎image omitted   | [‎, [, 01/09/2023, ,, 14:28:49, ], Barak, :, ‎... | [‎, [, 01/09/2023, ,, 14:28:49, ], Barak, :, ‎... | [‎, [, 01/09/2023, ,, 14:28:49, ], barak, :, ‎... |\n",
    "| [01/09/2023, 14:33:59] עמרי ששון : אוווו         | [[, 01/09/2023, ,, 14:33:59, ], עמרי, ששון, :,... | [[, 01/09/2023, ,, 14:33:59, ], עמרי, ששון, :,... | [[, 01/09/2023, ,, 14:33:59, ], עמרי, ששון, :,... |\n",
    "| [01/09/2023, 14:34:05] עמרי ששון : זה רשם אותך... | [[, 01/09/2023, ,, 14:34:05, ], עמרי, ששון, :,... | [[, 01/09/2023, ,, 14:34:05, ], עמרי, ששון, :,... | [[, 01/09/2023, ,, 14:34:05, ], עמרי, ששון, :,... |\n",
    "\n",
    "---\n",
    "\n",
    "## Task 11: Compare SpaCy Results (Tokenization, Lemmatization)\n",
    "\n",
    "| **Message**                                   | **SpaCy_Tokens**                                | **SpaCy_Lemmatized**                             |\n",
    "|-----------------------------------------------|------------------------------------------------|------------------------------------------------|\n",
    "| [01/09/2023, 13:48:44] Barak: Check out this j... | [[, 01/09/2023, ,, 13:48:44, ], Barak, :, Chec... | [[, 01/09/2023, ,, 13:48:44, ], Barak, :, chec... |\n",
    "| https://www.linkedin.com/jobs/view/3628574195 | [https://www.linkedin.com/jobs/view/3628574195] | [https://www.linkedin.com/jobs/view/3628574195] |\n",
    "| ‎[01/09/2023, 14:28:49] Barak: ‎image omitted   | [‎[01/09/2023, ,, 14:28:49, ], Barak, :, ‎imag... | [‎[01/09/2023, ,, 14:28:49, ], Barak, :, ‎imag... |\n",
    "| [01/09/2023, 14:33:59] עמרי ששון : אוווו         | [[, 01/09/2023, ,, 14:33:59, ], עמרי, ששון, :,... | [[, 01/09/2023, ,, 14:33:59, ], עמרי, ששון, :,... |\n",
    "| [01/09/2023, 14:34:05] עמרי ששון : זה רשם אותך... | [[, 01/09/2023, ,, 14:34:05, ], עמרי, ששון, :,... | [[, 01/09/2023, ,, 14:34:05, ], עמרי, ששון, :,... |\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "1. **Tokenization:** Differences mainly involve handling of special characters, URLs, and separators.\n",
    "2. **Lemmatization:** SpaCy aggressively lemmatizes, often reducing inflected forms to base forms.\n",
    "3. **Stemming:** NLTK applies more aggressive stemming than SpaCy.\n",
    "4. **Messages Affecting Tokens:** Tasks 8 and 9 identify rows where stemming and lemmatization affect token counts differently.\n",
    "\n",
    "The processed data is saved to `chat_processed_results.csv`.\n"
   ],
   "id": "c0d185fd08ca0aed"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
